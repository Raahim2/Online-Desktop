<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Wind Chime Visualization</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Minimal style for chime animation smoothness if Tailwind transitions aren't enough */
        .chime {
            transition: transform 0.2s ease-out, opacity 0.2s ease-out;
        }
    </style>
</head>
<body class="bg-gradient-to-br from-sky-100 via-blue-200 to-indigo-300 min-h-screen flex flex-col text-gray-800 font-sans">

    <header class="p-4 sm:p-6 shadow-md bg-white/80 backdrop-blur-sm">
        <div class="container mx-auto flex justify-between items-center">
            <h1 class="text-xl sm:text-2xl md:text-3xl font-bold text-indigo-700">Wind Chime Visualizer</h1>
            <p class="text-sm text-gray-600 hidden sm:block">Using your microphone input</p>
        </div>
    </header>

    <main class="flex-grow container mx-auto p-4 sm:p-8 flex flex-col items-center justify-center">

        <section class="text-center mb-8">
            <h2 class="text-2xl sm:text-3xl font-semibold mb-2 text-indigo-800">Listen to the Breeze</h2>
            <p class="text-gray-700 max-w-xl mx-auto" id="status-message">
                Click the button below to start the visualization. You'll need to grant microphone access. Sound picked up by your microphone will animate the chimes.
            </p>
        </section>

        <section class="w-full max-w-2xl h-64 sm:h-80 md:h-96 flex justify-center items-end gap-2 sm:gap-3 md:gap-4 p-4 bg-white/50 backdrop-blur-sm rounded-lg shadow-lg overflow-hidden" aria-label="Wind Chime Visualization Area">
            <!-- Chime Elements -->
            <div id="chime-1" class="chime w-4 sm:w-5 md:w-6 h-2/5 bg-gradient-to-b from-yellow-300 to-yellow-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-2" class="chime w-4 sm:w-5 md:w-6 h-3/5 bg-gradient-to-b from-orange-300 to-orange-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-3" class="chime w-4 sm:w-5 md:w-6 h-4/6 bg-gradient-to-b from-red-300 to-red-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-4" class="chime w-4 sm:w-5 md:w-6 h-3/4 bg-gradient-to-b from-purple-300 to-purple-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-5" class="chime w-4 sm:w-5 md:w-6 h-4/6 bg-gradient-to-b from-blue-300 to-blue-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-6" class="chime w-4 sm:w-5 md:w-6 h-3/5 bg-gradient-to-b from-teal-300 to-teal-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
            <div id="chime-7" class="chime w-4 sm:w-5 md:w-6 h-2/5 bg-gradient-to-b from-green-300 to-green-500 rounded-t-full rounded-b-md shadow-md origin-bottom" style="transform: scaleY(0.8); opacity: 0.8;"></div>
        </section>

        <section class="mt-8 text-center">
            <button id="startButton" class="px-6 py-3 bg-indigo-600 text-white font-semibold rounded-lg shadow-md hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-opacity-50 transition duration-150 ease-in-out">
                Start Visualization
            </button>
        </section>

    </main>

    <footer class="p-4 text-center text-sm text-gray-600 bg-white/30 backdrop-blur-sm mt-8">
        Powered by <a href="https://tailwindcss.com/" target="_blank" rel="noopener noreferrer" class="text-indigo-600 hover:underline">Tailwind CSS</a> & Web Audio API.
    </footer>

    <script>
        const startButton = document.getElementById('startButton');
        const statusMessage = document.getElementById('status-message');
        const chimes = document.querySelectorAll('.chime');

        let audioContext;
        let analyser;
        let microphone;
        let javascriptNode;
        let animationFrameId;

        const NUM_CHIMES = chimes.length;
        const BASE_SCALE = 0.8;
        const BASE_OPACITY = 0.8;
        const MAX_SCALE_FACTOR = 0.7; // Max additional scale (total max scale = BASE_SCALE + MAX_SCALE_FACTOR)
        const MAX_OPACITY_FACTOR = 0.2; // Max additional opacity (total max opacity = BASE_OPACITY + MAX_OPACITY_FACTOR)
        const SENSITIVITY = 1.5; // Adjust sensitivity to microphone input level

        function updateVisualization() {
            if (!analyser) return;

            const array = new Uint8Array(analyser.frequencyBinCount);
            analyser.getByteFrequencyData(array); // More sensitive to changes than time domain

            let sum = 0;
            for(let i = 0; i < array.length; i++) {
                sum += array[i];
            }
            const average = sum / array.length;

            // Normalize the average value (0-255) to a range (0-1) and apply sensitivity
            const normalizedLevel = Math.min(1, (average / 128) * SENSITIVITY); // Cap at 1

            // Update each chime based on the overall level
            chimes.forEach((chime, index) => {
                // Add some variation per chime - pseudo-randomness based on index
                const chimeFactor = (index % 3 + 1) / 3; // Simple variation factor (0.33, 0.66, 1.0)
                const levelForChime = normalizedLevel * chimeFactor;

                const scale = BASE_SCALE + (MAX_SCALE_FACTOR * levelForChime);
                const opacity = BASE_OPACITY + (MAX_OPACITY_FACTOR * levelForChime);

                // Apply styles - use transform for better performance
                chime.style.transform = `scaleY(${Math.min(scale, BASE_SCALE + MAX_SCALE_FACTOR)})`; // Ensure scale doesn't exceed max
                chime.style.opacity = `${Math.min(opacity, BASE_OPACITY + MAX_OPACITY_FACTOR)}`; // Ensure opacity doesn't exceed max
            });

            animationFrameId = requestAnimationFrame(updateVisualization);
        }

        function stopVisualization() {
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
            }
            if (microphone) {
                microphone.disconnect();
                microphone = null;
            }
            if (analyser) {
                analyser.disconnect();
                analyser = null;
            }
            if (javascriptNode) {
                javascriptNode.disconnect();
                 javascriptNode = null;
            }
             // Don't close the context immediately, might reuse
            // if (audioContext && audioContext.state !== 'closed') {
            //    audioContext.close();
            // }

            // Reset chimes to base state
             chimes.forEach(chime => {
                chime.style.transform = `scaleY(${BASE_SCALE})`;
                chime.style.opacity = `${BASE_OPACITY}`;
            });

            statusMessage.textContent = 'Visualization stopped. Click Start to begin again.';
            startButton.textContent = 'Start Visualization';
            startButton.disabled = false;
        }


        async function startAudioProcessing() {
            if (audioContext && audioContext.state === 'running') {
                 console.log("Audio context already running.");
                 updateVisualization(); // Restart animation loop if needed
                 return;
            }

            startButton.disabled = true;
            startButton.textContent = 'Initializing...';
            statusMessage.textContent = 'Requesting microphone access...';

            try {
                // Create AudioContext
                audioContext = new (window.AudioContext || window.webkitAudioContext)();

                // Resume context if suspended (often needed after user interaction)
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Get microphone input
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                microphone = audioContext.createMediaStreamSource(stream);

                // Create AnalyserNode
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256; // Lower size for faster processing, less detail

                // Create ScriptProcessorNode (deprecated but widely supported for basic level)
                // Note: AudioWorklet is the modern approach but more complex for this minimal example.
                const bufferSize = 1024;
                javascriptNode = audioContext.createScriptProcessor(bufferSize, 1, 1);

                // Connect nodes: Microphone -> Analyser -> ScriptProcessor -> Destination (optional, prevents echo)
                microphone.connect(analyser);
                analyser.connect(javascriptNode);
                javascriptNode.connect(audioContext.destination); // Connect to output to keep the node processing

                // Start processing audio data - the onaudioprocess event itself isn't strictly needed
                // if we use requestAnimationFrame driven by the analyser data.
                // javascriptNode.onaudioprocess = (audioProcessingEvent) => {
                    // Data is now available in the analyser node, processed in updateVisualization
                // };

                statusMessage.textContent = 'Listening... Make some noise!';
                startButton.textContent = 'Stop Visualization';
                startButton.disabled = false;

                // Start the visualization loop
                updateVisualization();

            } catch (err) {
                console.error('Error accessing microphone or setting up audio:', err);
                statusMessage.textContent = `Error: ${err.message}. Please ensure microphone access is granted.`;
                startButton.textContent = 'Start Visualization';
                startButton.disabled = false;
                // Clean up any partially created context
                 if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close();
                 }
                 audioContext = null;
            }
        }

        startButton.addEventListener('click', () => {
            if (audioContext && (audioContext.state === 'running' || animationFrameId)) {
                // If audio is running or visualization is active, stop it
                stopVisualization();
            } else {
                // Otherwise, start it
                startAudioProcessing();
            }
        });

    </script>

</body>
</html>